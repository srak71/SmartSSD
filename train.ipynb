{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7580c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d7b0b",
   "metadata": {},
   "source": [
    "The NEU-CLS dataset has 1800 grayscale images of steel surface defects (200×200 pixels) in six classes: rolled-in scale, patches, crazing, pitted surface, inclusion, and scratches.\n",
    "\n",
    "The dataset is organized into subdirectories per class in separate train/images and validation/images directories. Example: training images might be in .../train/images/scratches/ and similarly for the other classes. \n",
    "\n",
    "I would define my steps for training the classification model as follows:\n",
    "\n",
    "Part 1: Data Processing\n",
    "\n",
    "    1a. Proper loading of NEU data\n",
    "    1b. Augment training set and normalize both sets for unified comparison\n",
    "  \n",
    "Part 2: Model Creation and Training Parameters\n",
    "\n",
    "    2a. Defining Convolution Neural Net Model\n",
    "    2b. Establishing Callbacks to stop training when optimization met.\n",
    "  \n",
    "Part 3: Training and Selecting Optimal Model\n",
    "\n",
    "    3a. Training CNN with established parameters, and saving best model into 'SmartSSD/best_model.keras'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd4f242",
   "metadata": {},
   "source": [
    "# Part 1: Data Processing\n",
    "\n",
    "## 1a. Proper loading of NEU data.\n",
    "\n",
    "For reproducability I am using python's **os** package to establish given user's current working directory. Then joining with location of my train & validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47714c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user dir\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# known filepaths\n",
    "train_dir = os.path.join(current_dir, \"NEU-CLS\", \"train\", \"images\")\n",
    "val_dir   = os.path.join(current_dir, \"NEU-CLS\", \"validation\", \"images\")\n",
    "\n",
    "# unified variables\n",
    "image_size = (128, 128)\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "num_classes = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f04fd3",
   "metadata": {},
   "source": [
    "## 1b. Augment training set and normalize both sets for unified comparison\n",
    "\n",
    "For loading my images and labels I am using Keras package to read directories of images. My code uses TF's *ImageDataGenerator()* instead of the basic *image_dataset_from_directory()* for integrated data augmentation such as normalization (*rescale=1. / 255*) and transformations (to the training data only). By augmenting I am expanding the size of the dataset so my model has more images to train on without having to gather any new data. \n",
    "\n",
    "- Normalization: All images rescaled to be in range(0,1) by dividing by 225 and resizing to 128x128 resolution. *color_mode='greyscale'* specified so each image has one channel.\n",
    "\n",
    "- Augmentation: My transformations to the training data includes rotation and reflection (flip), as well as brightness transformation. Limiting range of augmentation to at most 20% for ensuring the image remains usable. No transformations/augmentations made to validation data to avoid leaking val information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7af7347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1440 images belonging to 6 classes.\n",
      "Found 360 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "# Augmenting training images by rotating flipping and altering brightness\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n",
    "                                                                rotation_range=20, # random rotations\n",
    "                                                                horizontal_flip=True, # random horizontal flips\n",
    "                                                                brightness_range=(0.8, 1.2)) # random brightness\n",
    "\n",
    "# just normalizing in the case of val images\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255) \n",
    "\n",
    "# actually generating the image defined from my ImageDataGenerator() function\n",
    "train_gen = train_datagen.flow_from_directory(train_dir,\n",
    "                                              target_size=image_size,\n",
    "                                              color_mode='grayscale',\n",
    "                                              batch_size=batch_size,\n",
    "                                              class_mode='categorical')\n",
    "\n",
    "val_gen = val_datagen.flow_from_directory(val_dir,\n",
    "                                          target_size=image_size,\n",
    "                                          color_mode='grayscale',\n",
    "                                          batch_size=batch_size,\n",
    "                                          class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37800c92",
   "metadata": {},
   "source": [
    "# Part 2. Defining Convolution Neural Net Model\n",
    "\n",
    "## 2a. I will create a basic **Convolutional Neural Network (CNN)** consisting of three blocks and a classification head. \n",
    "\n",
    "Block 1, 2, 3: \n",
    "\n",
    "> Layer 1. Convolution layer with **ReLu** activation function. *Conv2D()*\n",
    ">\n",
    "> Layer 2. Establishing limitations to my batch pool_size. *MaxPooling2D()*\n",
    ">\n",
    "> Layer 3. *Dropout()* layer limiting overfitting (also normalizing).\n",
    "\n",
    "Classification Head:\n",
    "\n",
    "> Layer 1: *Flatten()* layer to  the feature maps.\n",
    ">\n",
    "> Layer 2: *Dense()* layer using **ReLU** as my activation.\n",
    "> \n",
    "> Layer 3: *Dropout()* layer to limit overfitting model.\n",
    ">\n",
    "> Layer 4: *Dense()* layer but now using number of types of defects (classes), thus switch to **softmax** activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95c8a603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    # Block 1\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation= 'relu', input_shape= (128, 128, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    \n",
    "    # Block 2\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation= 'relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    \n",
    "    # Block 3\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation= 'relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    \n",
    "    # Classification head\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation= 'relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(num_classes, activation= 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf64642",
   "metadata": {},
   "source": [
    "Finally, my last block will compile my established CNN. Since there are multiple different classifications possible for image, I will compile model with **Adam** as my optimizer and measure by categorical cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c644fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,211,392</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">774</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m3,211,392\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m774\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,304,838</span> (12.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,304,838\u001b[0m (12.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,304,838</span> (12.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,304,838\u001b[0m (12.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ba45d9",
   "metadata": {},
   "source": [
    "## 2b. Establishing Callbacks to stop training when optimization met.\n",
    "\n",
    "I will train my model with the entire dataset (*epochs=*) 10 times. While fitting the model using *model.fit()* in Part 3, Keras autoprints training and validation accuracy/loss for every epoch. I will set *callbacks=* 'EarlyStopping' and 'ModelCheckpoint' in *model.fit()* to accomplish the following:\n",
    "\n",
    "- EarlyStopping: While monitoring training loss, I will stop training if model performance does not improve for 3 epochs. For instance, if epoch 2 has val_loss of 0.1 (hypothetically), and epochs 3, 4, 5 have val_loss greater than 0.1, training should stop after epoch 5, and the model weights from epoch 2 should be loaded.\n",
    "\n",
    "- ModelCheckpoint: I save model weights to file **best_model.keras** whenever I get improvement of performance on the validation set by setting *save_best_only=* True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40b82715",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStop = tf.keras.callbacks.EarlyStopping(monitor= 'val_loss',\n",
    "                                             patience= 3, # test for improvement in next 3, if none stop and load old\n",
    "                                             restore_best_weights= True,\n",
    "                                             verbose= 1)\n",
    "\n",
    "# after stopping loading weights from old epoch (the one with the lowest val loss)\n",
    "modelCheckpoint = tf.keras.callbacks.ModelCheckpoint('best_model.keras',\n",
    "                                                     monitor= 'val_loss',\n",
    "                                                     save_best_only= True,\n",
    "                                                     verbose= 1)\n",
    "\n",
    "callbacks = [earlyStop, modelCheckpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d17dd7",
   "metadata": {},
   "source": [
    "# Part 3: Training and Selecting Optimal Model\n",
    "\n",
    "## 3a. Training and saving my best performing model to 'SmartSSD/best_model.keras'.\n",
    "\n",
    "In order to use the trained model for classification on new data, I will save it to my directory in file **best_model.keras**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "219db9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.1654 - loss: 1.9149\n",
      "Epoch 1: val_loss improved from inf to 1.77510, saving model to best_model.keras\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 82ms/step - accuracy: 0.1656 - loss: 1.9130 - val_accuracy: 0.1667 - val_loss: 1.7751\n",
      "Epoch 2/10\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.2282 - loss: 1.7270\n",
      "Epoch 2: val_loss improved from 1.77510 to 1.72828, saving model to best_model.keras\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 81ms/step - accuracy: 0.2284 - loss: 1.7266 - val_accuracy: 0.4278 - val_loss: 1.7283\n",
      "Epoch 3/10\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3529 - loss: 1.6059\n",
      "Epoch 3: val_loss improved from 1.72828 to 1.70060, saving model to best_model.keras\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 80ms/step - accuracy: 0.3532 - loss: 1.6052 - val_accuracy: 0.3000 - val_loss: 1.7006\n",
      "Epoch 4/10\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.4321 - loss: 1.4518\n",
      "Epoch 4: val_loss improved from 1.70060 to 1.38632, saving model to best_model.keras\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 81ms/step - accuracy: 0.4326 - loss: 1.4510 - val_accuracy: 0.5000 - val_loss: 1.3863\n",
      "Epoch 5/10\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.5256 - loss: 1.2455\n",
      "Epoch 5: val_loss improved from 1.38632 to 1.20708, saving model to best_model.keras\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 82ms/step - accuracy: 0.5265 - loss: 1.2433 - val_accuracy: 0.5694 - val_loss: 1.2071\n",
      "Epoch 6/10\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.7116 - loss: 0.8710\n",
      "Epoch 6: val_loss improved from 1.20708 to 1.11164, saving model to best_model.keras\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 81ms/step - accuracy: 0.7113 - loss: 0.8698 - val_accuracy: 0.5500 - val_loss: 1.1116\n",
      "Epoch 7/10\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.7571 - loss: 0.6850\n",
      "Epoch 7: val_loss improved from 1.11164 to 0.79921, saving model to best_model.keras\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 81ms/step - accuracy: 0.7572 - loss: 0.6847 - val_accuracy: 0.6833 - val_loss: 0.7992\n",
      "Epoch 8/10\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.7771 - loss: 0.6227\n",
      "Epoch 8: val_loss improved from 0.79921 to 0.59886, saving model to best_model.keras\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 81ms/step - accuracy: 0.7771 - loss: 0.6226 - val_accuracy: 0.7611 - val_loss: 0.5989\n",
      "Epoch 9/10\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.8028 - loss: 0.5568\n",
      "Epoch 9: val_loss did not improve from 0.59886\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 80ms/step - accuracy: 0.8033 - loss: 0.5558 - val_accuracy: 0.5250 - val_loss: 1.9361\n",
      "Epoch 10/10\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.8287 - loss: 0.4684\n",
      "Epoch 10: val_loss did not improve from 0.59886\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 80ms/step - accuracy: 0.8287 - loss: 0.4685 - val_accuracy: 0.5611 - val_loss: 1.4342\n",
      "Restoring model weights from the end of the best epoch: 8.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_gen,\n",
    "                    epochs= epochs,\n",
    "                    validation_data= val_gen,\n",
    "                    callbacks= callbacks,\n",
    "                    verbose= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062d5ae6",
   "metadata": {},
   "source": [
    "Even though my models val set accuracy is comparable in epochs 4 and 7, we can see that validation loss is significantly greater for epoch 7. Thus I decided to retrain with a higher early stop patience. Now I will continue testing for 5 values after find local maxima. I will still monitor validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c320cd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.8048 - loss: 0.5332\n",
      "Epoch 1: val_loss improved from inf to 0.90984, saving model to best_model.keras\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 83ms/step - accuracy: 0.8046 - loss: 0.5339 - val_accuracy: 0.6583 - val_loss: 0.9098\n",
      "Epoch 2/10\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.8000 - loss: 0.5639\n",
      "Epoch 2: val_loss improved from 0.90984 to 0.77774, saving model to best_model.keras\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 81ms/step - accuracy: 0.7995 - loss: 0.5659 - val_accuracy: 0.7306 - val_loss: 0.7777\n",
      "Epoch 3/10\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.7366 - loss: 0.8488\n",
      "Epoch 3: val_loss improved from 0.77774 to 0.70642, saving model to best_model.keras\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 81ms/step - accuracy: 0.7379 - loss: 0.8442 - val_accuracy: 0.7056 - val_loss: 0.7064\n",
      "Epoch 4/10\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8525 - loss: 0.4533\n",
      "Epoch 4: val_loss did not improve from 0.70642\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 79ms/step - accuracy: 0.8521 - loss: 0.4544 - val_accuracy: 0.7000 - val_loss: 0.7198\n",
      "Epoch 5/10\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.8848 - loss: 0.3953\n",
      "Epoch 5: val_loss did not improve from 0.70642\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 80ms/step - accuracy: 0.8848 - loss: 0.3945 - val_accuracy: 0.6833 - val_loss: 0.8619\n",
      "Epoch 6/10\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.8545 - loss: 0.4214\n",
      "Epoch 6: val_loss did not improve from 0.70642\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 80ms/step - accuracy: 0.8549 - loss: 0.4207 - val_accuracy: 0.6361 - val_loss: 1.0486\n",
      "Epoch 7/10\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8904 - loss: 0.3239\n",
      "Epoch 7: val_loss did not improve from 0.70642\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 79ms/step - accuracy: 0.8903 - loss: 0.3240 - val_accuracy: 0.6306 - val_loss: 1.5941\n",
      "Epoch 8/10\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.8850 - loss: 0.3344\n",
      "Epoch 8: val_loss did not improve from 0.70642\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 79ms/step - accuracy: 0.8852 - loss: 0.3342 - val_accuracy: 0.5194 - val_loss: 1.9389\n",
      "Epoch 8: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n"
     ]
    }
   ],
   "source": [
    "earlyStop = tf.keras.callbacks.EarlyStopping(monitor= 'val_loss',\n",
    "                                             patience= 5, # test for improvement in next 3, if none stop and load old\n",
    "                                             restore_best_weights= True,\n",
    "                                             verbose= 1\n",
    "                                             )\n",
    "\n",
    "# after stopping loading weights from old epoch (the one with the lowest val loss)\n",
    "modelCheckpoint = tf.keras.callbacks.ModelCheckpoint('best_model.keras',\n",
    "                                                     monitor= 'val_loss',\n",
    "                                                     save_best_only= True,\n",
    "                                                     verbose= 1\n",
    "                                                     )\n",
    "\n",
    "callbacks = [earlyStop, modelCheckpoint]\n",
    "\n",
    "history = model.fit(train_gen,\n",
    "                    epochs= epochs,\n",
    "                    validation_data= val_gen,\n",
    "                    callbacks= callbacks,\n",
    "                    verbose= 1\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7d06f1",
   "metadata": {},
   "source": [
    "Our new validation loss is considerably better (0.4458) and now selects epoch 7 as the optimal. Additionally, we can see significant improvement in validation set classification accuracy (0.83) compared to our previous selection of epoch 4 (0.64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8764442e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
